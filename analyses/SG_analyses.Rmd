---
title: "SG_analyses"
author: "Sabrina Grant"
date: "`r Sys.Date()`"
output: html_document
---

#packages
```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
library(stats)
library(dplyr)
library(janitor)
library(nlme)
library(RColorBrewer)
library(egg)
library(vegan)
library(librarian)
library(splitstackshape)
library(lubridate)
library(matrixStats)
library(BiodiversityR)
library(ggforce)
library(concaveman)
library(ggrepel)
library(ggsci)
library(pvclust)
library(googlesheets4)
library(stringr)
library(readr)

```

```{r}
rm(list=ls())
require(tidyverse)  
require(here)

load(here::here("output","processed", "cleaned_survey_data.Rda")) 
```


#Fixing up data frames
```{r}
###averaging counts and % cover per transect

# Helper function to get the most common value (mode)
get_mode <- function(x) {
  ux <- na.omit(unique(x))
  ux[which.max(tabulate(match(x, ux)))]
}

quad_transect_avgs <- quad_data %>%
  group_by(site, site_type, zone, survey_date, transect) %>%
  summarise(
    across(where(is.numeric) & !any_of("quadrat"), ~ mean(.x, na.rm = TRUE)),
    substrate = get_mode(substrate),
    .groups = "drop"
  )


# View result
print(quad_transect_avgs)

#now averaging per survey 
quad_avgs <- quad_transect_avgs %>%
  group_by(site, site_type, zone, survey_date) %>%
  summarise( 
    across(where(is.numeric) & !any_of("transect"), ~mean(.x, na.rm = TRUE)), 
    substrate = get_mode(substrate), 
    .groups = "drop")

# View result
print(quad_avgs)

#now making a new column to represent urchin behavior i.e. how many urchins are actively grazing 

quad_avgs1 <- quad_avgs %>%
  mutate( purple_urchin_active_foraging_densitym2 = purple_urchin_densitym2 - purple_urchin_conceiledm2,
    red_urchin_active_foraging_densitym2 = red_urchin_densitym2 - red_urchin_conceiledm2
  ) %>%
  mutate(total_urchin_densitym2 = purple_urchin_densitym2 + red_urchin_densitym2, 
         total_urchin_foraging_densitym2 = purple_urchin_active_foraging_densitym2 + red_urchin_active_foraging_densitym2)
  #separate purps and reds, start with just purps

#averaging transects
kelp_avgs <- kelp_data %>%
  group_by(site, site_type, zone, survey_date) %>%
  summarise( 
    across(where(is.numeric) & !any_of("transect"), ~mean(.x, na.rm = TRUE)), 
    .groups = "drop") 

macro_avgs <- macro_data %>%
  group_by(site, site_type, zone, survey_date) %>%
  summarise( 
    across(where(is.numeric) & !any_of("transect"), ~mean(.x, na.rm = TRUE)), 
    .groups = "drop")


```



#Merging Quad/Kelp/Macro Dfs

```{r}
merged_data <- kelp_avgs %>%
  mutate(site = if_else(site == "RECO_04", "REC_04", site)) %>%
  left_join(quad_avgs1, by = c("site", "site_type", "zone", "survey_date")) %>%
  left_join(macro_avgs, by = c("site","site_type", "zone", "survey_date"))

#creating a new column that just has years for future analysis  
merged_data$year <- as.numeric(format(merged_data$survey_date, "%Y"))
  

print(merged_data)
```

#2024 data 
```{r}
#normalizing 2024 data 

year_one_data <- merged_data %>% 
  filter(year == 2024) %>%
  filter() %>%
  dplyr::select(year, site, zone, site_type, macro_stipe_density_20m2, everything())

year_one_data[sapply(year_one_data, is.numeric)] <-
  lapply(year_one_data[sapply(year_one_data, is.numeric)],
         function(x) { x[is.na(x)] <- 0; x })

year_one_data %>%
  group_by(site, site_type, zone, year) %>%
  summarise(across(where(is.numeric), mean, na.rm = TRUE), .groups = "drop")
  
```


##LDA at the depth zone level
```{r}
library(MASS)

year_one_data$site_type <- as.factor(year_one_data$site_type)
  
# Identify columns that contain "20m2"
cols_to_scale <- grep("20m2", names(year_one_data), value = TRUE)

# Divide those columns by 20
year_one_data[cols_to_scale] <- lapply(year_one_data[cols_to_scale], function(x) x / 20)

# 3. Rename those columns by removing "20m2_"
names(year_one_data)[names(year_one_data) %in% cols_to_scale] <-
  gsub("20m2","", cols_to_scale)

lda_model <- lda(site_type ~ macro_stipe_density_ + density_ptecal + density_nerlue + density_lamset + density_eisarb + purple_urchin_densitym2 + purple_urchin_conceiledm2 + lamr_densitym2 + macr_densitym2 + macj_densitym2 + nerj_densitym2 + ptej_densitym2 + lsetj_densitym2  + tegula_densitym2 + cov_articulated_coralline + cov_crustose_coralline + cov_encrusting_red + cov_fleshy_red + cov_stephanocystis + cov_dictyoneurum_spp + cov_dictyota_dictyopteris + cov_desmarestia_spp+ cov_lam_holdfast_live + cov_mac_holdfast_live, data = year_one_data)



#lda figure
plot(lda_model)

#lda summary
lda_model


#getting predictions etc etc 
pred <- predict(lda_model)

pred_class <- pred$class

results <- year_one_data %>%
  mutate(predicted = pred_class, 
         match = site_type == predicted)

results <- results %>%
  dplyr::select(year, site, zone, site_type, predicted, match, everything())

mismatches <- results %>% filter(!match)

mismatches

results 

#ACCURACY = 93.55%
library(caret)
confusionMatrix(pred_class, year_one_data$site_type)


```
###coefficients of LDA
```{r}
library(tidyr)

# Get coefficients into tidy format
coef_df <- as.data.frame(lda_model$scaling)
coef_df$variable <- rownames(coef_df)
coef_long <- pivot_longer(coef_df, cols = c(LD1, LD2), names_to = "LD", values_to = "coefficient")

# Plot
ggplot(coef_long, aes(x = reorder(variable, coefficient), y = coefficient, fill = LD)) +
  geom_bar(stat = "identity", position = "dodge") +
  coord_flip() +
  theme_minimal(base_size = 14) +
  labs(
    title = "LDA Coefficients by Discriminant Function",
    x = "Variable",
    y = "Coefficient"
  ) +
  scale_fill_brewer(palette = "Set1")

```


```{r}

# Get LDA scores
lda_scores <- as.data.frame(predict(lda_model)$x)
lda_scores$site_type <- year_one_data$site_type

# Plot LD1 vs LD2
ggplot(lda_scores, aes(x = LD1, y = LD2, color = site_type)) +
  geom_point(size = 3, alpha = 0.8) +
  stat_ellipse(level = 0.95, linetype = 2) +
  theme_minimal(base_size = 14) +
  labs(
    title = "LDA: LD1 vs LD2",
    x = paste0("LD1 (", round(lda_model$svd[1]^2 / sum(lda_model$svd^2), 2) * 100, "%)"),
    y = paste0("LD2 (", round(lda_model$svd[2]^2 / sum(lda_model$svd^2), 2) * 100, "%)")
  )
```



#2025 data
##creating new df
```{r}
#filtering to just select for 2025 data 
year_two_data <- merged_data %>% 
  filter(year == 2025) %>%
  filter() %>%
  dplyr::select(year, site, zone, site_type, macro_stipe_density_20m2, everything())

  
# Identify columns that contain "20m2"
cols_to_scale <- grep("20m2", names(year_two_data), value = TRUE)

# Divide those columns by 20
year_two_data[cols_to_scale] <- lapply(year_two_data[cols_to_scale], function(x) x / 20)

# 3. Rename those columns by removing "20m2_"
names(year_two_data)[names(year_two_data) %in% cols_to_scale] <-
  gsub("20m2","", cols_to_scale)


year_two_data[sapply(year_two_data, is.numeric)] <-
  lapply(year_two_data[sapply(year_two_data, is.numeric)],
         function(x) { x[is.na(x)] <- 0; x })

year_two_data %>%
  group_by(site, site_type, zone, year) %>%
  summarise(across(where(is.numeric), mean, na.rm = TRUE), .groups = "drop")
  

print(year_two_data)
```

##using 2024 LDA to predict 2025
```{r}

library(MASS)
library(dplyr)

#predicting 2025 patch types based on model
pred_2025 <- predict(lda_model, newdata = year_two_data)

year_two_data <- year_two_data %>%
  dplyr::mutate(site_type_pred = pred_2025$class) %>%
  dplyr::select(year, site, zone, site_type, site_type_pred, dplyr::everything())


#setting up the 2024 df with the predictions column 
pred_2024 <- predict(lda_model, newdata = year_one_data)
year_one_data$site_type_pred <- pred_2024$class

year_one_data <- year_one_data %>%
  dplyr::mutate(site_type_pred = pred_2024$class) %>%
  dplyr::select(year, site, zone, site_type, site_type_pred, dplyr::everything())


##REJOINING 2024 AND 2025 DFS

# Rename the column first
year_one_renamed <- year_one_data %>%
  dplyr::rename(site_type_pred_2024 = site_type_pred) %>%
  dplyr::select(site, zone, site_type, site_type_pred_2024)

year_two_renamed <- year_two_data %>%
  dplyr::rename(site_type_pred_2025 = site_type_pred) %>%
  dplyr::select(site, zone, site_type, site_type_pred_2025)

#merge
merged <- dplyr::left_join(year_one_renamed, year_two_renamed, by = c("site", "zone", "site_type"))


#cross-tabulate 2024 predictions with 2025
library(dplyr)

merged <- year_one_data %>%
  dplyr::select(site, zone, site_type, site_type_pred_2024 = site_type_pred) %>%
  left_join(
    year_two_data %>%
      dplyr::select(site, zone, site_type, site_type_pred_2025 = site_type_pred),
    by = c("site", "zone", "site_type")
  )


cross_tab <- table(merged$site_type_pred_2024, merged$site_type_pred_2025)

```

##Heatmap of Predicted Patch Transitions
```{r}
library(ggplot2)
library(reshape2)
library(viridis)

# Melt the table
cross_tab_melt <- reshape2::melt(cross_tab)

# changing the order of the variables for my plot
desired_order_2024 <- c("BAR", "INCIP", "FOR") 
desired_order_2025 <- c("BAR", "INCIP", "FOR")  

# Convert to factors with specified levels
cross_tab_melt$Var1 <- factor(cross_tab_melt$Var1, levels = desired_order_2024)
cross_tab_melt$Var2 <- factor(cross_tab_melt$Var2, levels = desired_order_2025)

  

# Sleek publication-ready heatmap
ggplot(cross_tab_melt, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile(color = "grey90", linewidth  = 0.3) +  # subtle grid lines
  geom_text(aes(label = value), 
            color = ifelse(cross_tab_melt$value > max(cross_tab_melt$value)/2, "white", "black"), 
            size = 5, fontface = "bold") +   # dynamic label color
scale_fill_manual(values="#ffffcc","#c7e9b4", "#7fcdbb", "#41b6c4", "#2c7fb8", "#253494")+
  labs(
    x = "2024",
    y = "2025",
    title = "Transition of Predicted Patch Types from 2024 to 2025"
  ) +
  theme_minimal(base_size = 15) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, face = "bold", size = 13),
    axis.text.y = element_text(face = "bold", size = 13),
    axis.title = element_text(face = "bold", size = 14),
    plot.title = element_text(face = "bold", hjust = 0.5, size = 18, margin = margin(b = 10)),
    legend.title = element_text(face = "bold", size = 13),
    legend.text = element_text(size = 12),
    panel.grid = element_blank()  # remove minor grid for sleek look
  )



```

```{r}
library(dplyr)
library(reshape2)
library(ggplot2)
library(viridis)

# Make sure your data has site_type_pred_2024, site_type_pred_2025, and zone
# Create a summarized table per zone
cross_tab_zone <- merged %>%
  dplyr::group_by(zone, site_type_pred_2024, site_type_pred_2025) %>%
  dplyr::summarise(count = n(), .groups = "drop")

# Plot faceted heatmap
ggplot(cross_tab_zone, aes(x = site_type_pred_2024, y = site_type_pred_2025, fill = count)) +
  geom_tile(color = "grey90", size = 0.3) +
  geom_text(aes(label = count),
            color = ifelse(cross_tab_zone$count > max(cross_tab_zone$count)/2, "white", "black"),
            size = 4, fontface = "bold") +
  scale_fill_viridis(option = "C", direction = -1, name = "Count") +
  labs(
    x = "Predicted 2024 Patch Type",
    y = "Predicted 2025 Patch Type",
    title = "Transition of Predicted Patch Types by Zone"
  ) +
  theme_minimal(base_size = 13) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, face = "bold", size = 12),
    axis.text.y = element_text(face = "bold", size = 12),
    axis.title = element_text(face = "bold", size = 13),
    plot.title = element_text(face = "bold", hjust = 0.5, size = 16, margin = margin(b = 10)),
    legend.title = element_text(face = "bold", size = 12),
    legend.text = element_text(size = 11),
    panel.grid = element_blank()
  ) +
  facet_wrap(~ zone, scales = "free")  # facets by zone

```




